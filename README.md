<div align="center">
<figure>
    <img src="https://upload.wikimedia.org/wikipedia/commons/7/79/1960s_Gucci_Logo.svg" width="100" height="70">
</figure>
<h1>GVCCI: Lifelong Learning of Visual Grounding <br> for Language-Guided Robotic Manipulation</h1>
  
**[Junghyun Kim][4], &nbsp; [Gi-Cheon Kang][3]<sup>\*</sup>, &nbsp; [Jaein Kim][5]<sup>\*</sup>, &nbsp; [Suyeon Shin][7], &nbsp; [Byoung-Tak Zhang][6]** <br>

**[The 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023)][2]**
</div>

<h3 align="center">
<a href="https://arxiv.org/abs/2307.05963">arXiv</a> | <a href="https://drive.google.com/file/d/1QO_ElVKPAxTQo6-pmuHihGgp23WkyBjp/view?usp=sharing">Poster</a> | <a href="https://drive.google.com/file/d/1PeWlImlTqXNKS1N0li2fO9xptHlozCVq/view?usp=sharing">Presentation Video</a> | <a href="https://drive.google.com/file/d/1DfGMTGrifpXlsp_0Y2XbxmNR4VDlRM1u/view?usp=sharing">Demo Video</a>
</h3>

## Overview
<img src="GVCCI_demo_video.gif" width="100%" align="middle"><br><br>

Citation
-----------------------------
If you use this code or data in your research, please consider citing:
```bibtex
@article{kim2023gvcci,
  title={Gvcci: Lifelong learning of visual grounding for language-guided robotic manipulation},
  author={Kim, Junghyun and Kang, Gi-Cheon and Kim, Jaein and Shin, Suyeon and Zhang, Byoung-Tak},
  journal={arXiv preprint arXiv:2307.05963},
  year={2023}
}
```

<!--
## Table of Contents
* [Setup and Dependencies](#Setup-and-Dependencies)
* [Download Data](#Download-Data)
* [Pre-trained Checkpoints](#Pre-trained-Checkpoints)
* [Training](#Training)
* [Adaptation to Discriminative Visual Dialog](#Adaptation-to-Discriminative-Visual-Dialog)
* [Visual Dialog Generation](#Visual-Dialog-Generation)
* [Evaluation](#Evaluation)
* [Adversarial Robustness Study](#Adversarial-Robustness-Study)
* [Demo](#Demo)
* [Acknowledgements](#Acknowledgements)
* [License](#License)<br><br>
-->

Environment Setup
----------------------
Python 3.7+, PyTorch v1.9.1+, CUDA 11+ and CuDNN 7+, Anaconda/Miniconda (recommended) <br>

1. Install Anaconda or Miniconda from [here][8].
2. Clone this repository and create an environment:

```shell
git clone https://www.github.com/JHKim-snu/GVCCI
conda create -n gvcci python=3.8
conda activate gvcci
```

3. Install all dependencies:
```shell
pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html
pip install -r requirements.txt
```

VGPI Dataset
----------------------
VGPI (Visual Grounding on Pick-and-place Instruction) is a visual grounding dataset collected from two distinct robotic environments ENV1 and ENV2.
While training set only consists of raw images of the environment, each sample in test set consists of:

1) images containing multiple objects
2) natural language pick-and-place instructions
3) bbox coordinates of the corresponding objects

We also provide a data generated by GVCCI.

**ENV1**
| Name  | Content | Examples | Size | Link |
| --- | --- |--- | --- |--- |
| `ENV1_train.zip`  | ENV1 training set images | 540 | 25.8 MBytes | [Download](https://drive.google.com/file/d/1GWiUB2ZSOPdVfoV9JAbmiFbINvVLZSdS/view?usp=drive_link)|
| `ENV1_generated_samples.pth`  | Sample generated by GVCCI | 97,448 | 7 MBytes | [Download](https://drive.google.com/file/d/1nzzx4t275kLaYimqu-lMpsZXjsCrByE4/view?usp=sharing)|
| `Test-H.tsv`  | Test-H | 212 | 28.4 MBytes | [Download](https://drive.google.com/file/d/148G5AqU2JTLlkSRJZatGVE5bfYWK7fCi/view?usp=sharing)|
| `Test-R.tsv`  | Test-R | 180 | 22.6 MBytes | [Download](https://drive.google.com/file/d/1_S-uQtx0Hd5JD67rdiC8LpAuFLmBzoro/view?usp=sharing)|

**ENV2**
| Name  | Content | Examples | Size | Link |
| --- | --- |--- | --- |--- |
| `ENV2_train.zip`  | ENV2 training set images | 135 | 47.3 MBytes | [Download](https://drive.google.com/file/d/1ULsvwc8YY2xkTL7QaHznumpPSRJ1bSa-/view?usp=sharing)|
| `ENV2_generated_samples.pth`  | Sample generated by GVCCI | 19,511 | 1.4 MBytes | [Download](https://drive.google.com/file/d/1LlaVqFczyouyZVGXSc8WitfACj6ZcfRu/view?usp=sharing)|
| `Test-E.zip`  | Test-E images | 30 | 10.1 MBytes | [Download](https://drive.google.com/file/d/1Ui9VI3CMntAFsoWIvTvdQWbpCuGx2wQk/view?usp=sharing)|
| `Test-E.pth`  | Test-E instructions | 68 | 7 KBytes | [Download](https://drive.google.com/file/d/1RJjx2oIEhjKUw4deBmXMVwqcGB3lPmjV/view?usp=sharing)|

Each line in Test-H and Test-R represents a sample with the unique-id, image-id, pick instruction, bbox-coordinates, and image base64 string sepearated by tabs as shown below.

<pre>
103	0034	pick the bottle on the right side of the yellow can	175.64337349397593,45.79014989293362,195.3831325301205,85.2591006423983	iVBORw0KGgoAAAANSUhE...
</pre>

Each element in ENV1_generated_samples.pth, ENV2_generated_samples.pth, and Test-E.pth consists of the name of the image file in Test-E.zip, bbox coordinates, and pick instruction as shown below.

<pre>
['0001.png', '',[104.94623655913979,  88.6021505376344,  196.12903225806454,  170.32258064516128], 'pick the green cup in behind', '']
</pre>

Place the data in `./data` folder.
We expect data to be uploaded to the following directory structure:

    ├── data         
    │   ├── train       
    │   │   ├── ENV1_train
    │   │   │   ├── 0000.png      
    │   │   │   └── ...      
    │   │   ├── ENV2_train   
    │   │   │   ├── 0000.png      
    │   │   │   └── ...      
    │   ├── test  
    │   │   ├── Test-H.tsv  
    │   │   ├── Test-R.tsv  
    │   │   ├── Test-E.pth  
    │   │   ├── Test-E  
    │   │   │   ├── 0000.png
    │   │   │   └── ...      
    └── 

Visual Feature Extraction
--------------------------------------
Once you recieve images from whereever (robot, web, etc.), you first need to extract visual features of objects (category, attribute, location) in images to generate the instructions.
For visual feature extraction, we leverage the pretrained classifiers and object detector from [Faster R-CNN](https://arxiv.org/abs/1506.01497) and [Bottom-Up Attention](https://arxiv.org/abs/1707.07998).
The code is originated and modified from [this repository](https://github.com/MILVLG/bottom-up-attention.pytorch).
We strongly recommend you to use a separate environment for the visual feature extraction.
Please follow the Prerequisites [here](https://github.com/MILVLG/bottom-up-attention.pytorch).

Extract the visual features with the following script:
```shell
cd visual_feature_extraction
python make_image_list.py
OMP_NUM_THREADS=4 CUDA_VISIBLE_DEVICES=0,1,2,3 python extract.py --load_dir ./output_caffe152/ --image_dir ../data/train/ENV1_train/ --out_path ../instruction_generation/data/detection_results/ENV1/r152_attr_detection_results --image_list_file ./ENV1_train_train_imagelist_split0.txt --vg_dataset ENV1_train --cuda --split_ind 0
```

The extracted visual feature should be saved as following:

    ├── instruction_generation        
    │   ├── data        
    │   │   ├── detected_results
    │   │   │   ├── ENV1_train   
    │   │   │   │   ├── r101_object_detection_results
    │   │   │   │   │   ├── ENV1_train_train_pseudo_split0_detection_results.pth
    │   │   │   │   ├── r152_attr_detection_results      
    │   │   │   │   │   ├── ENV1_train_train_pseudo_split0_attr_detection_results.pth

The results will be a dictionary of name of the image file for keys and list of each object's features for values.


Instruction Generation
--------------------------------------

Now, you are ready to generate an instruction based on the extracted features.
Generating instructions can be performed with the following script:

```shell
bash scripts/generate_pseudo_data.sh
```

Generated data will be saved in `.pth` format which is a list of sample instructions.
Each sample instruction is a list that consists of 

1. image file name
2. object location
3. instruction


Visual Grounding
--------------------------------------
Since you have a generated triplet of image, location, and instructions, you can train any visual grounidng model. 
Here, we provide a sample training and evaluation code of [OFA](http://arxiv.org/abs/2202.03052).


Language-Guided Robotic Manipulation
--------------------------------------


Experimental Results
--------------------------------------




Checkpoints of GVCCI can be found below.

**GVCCI checkpoints**
| ENV1(8)  | ENV1(33) | ENV1(135) | ENV1(540) | ENV2(8) | ENV2(33) | ENV2(135) |
| --- | --- | --- | --- | --- | --- | --- |
| [Download]()| [Download]() | [Download]() | [Download]() | [Download]() | [Download]() | [Download]() |




Acknowledgements
-----------------
This repo is built on [Bottom-Up Attention](https://github.com/MILVLG/bottom-up-attention.pytorch), [Pseudo-Q](https://github.com/LeapLabTHU/Pseudo-Q), [OFA](https://github.com/OFA-Sys/OFA), and [MDETR](https://github.com/ashkamath/mdetr).


<!--

Pre-trained Checkpoints
--------------------------------------
Please download the checkpoints to `checkpoints/` directory.

| Model | Trained Data | Link |
|:-------:|:---------:|:------:|
|Questioner | VisDial v1.0 |[Download](https://www.dropbox.com/s/y3jdbfdaccholu3/questioner_v1.0.ckpt)|
|Teacher | VisDial v1.0 | [Download](https://www.dropbox.com/s/5p4wyjb0hny691m/teacher_v1.0.ckpt)|
|Student | VisDial v1.0 + CC12M with Synthetic Dialogs (iter3)| [Download](https://www.dropbox.com/s/ycgbn2mh0mtoktv/student_v1.0_iter3.ckpt)|
|Student (Discriminative)| VisDial v1.0 + CC12M with Synthetic Dialogs (iter3)| [Download](https://www.dropbox.com/s/uxnlknhgzr8f8tq/student_v1.0_iter3_disc_dense.ckpt)
|Base Model from [VisDial-BERT][10]| CC3M + VQA | [Download](https://www.dropbox.com/s/g38qemmqep1tt1a/basemodel)|


Training
--------
Teacher model and questioner model training. Nearly 54G gpu memory is required to train the model. The argument `-enc_dec_a` denotes an encoder-decoder model for answerer model, and `-enc_dec_q` is the encoder-decoder model for questioner model.  
```shell
# Teacher model training
python train_gen.py \
  -mode vd_train \
  -start_path checkpoints/basemodel \
  -model enc_dec_a \
  -gpu_ids 0 1 2 3
```
```shell
# Questioner model training
python train_gen.py \
  -mode vd_train \
  -start_path checkpoints/basemodel \
  -model enc_dec_q \
  -gpu_ids 0 1 2 3
```

Student model training consists of two steps: (1) training on synthetically generated visual dialog dataset and (2) finetuning on original visual dialog dataset. The argument `-chunk` denotes the number of data chunk to use (default 30). `-select_data` is to use perplexity-based data selection method. After training on the synthetic dialog data, the student model is trained on the original visual dialog data. 
```shell
# training a synthetic visual dialog dataset
python train_gen.py \
  -mode cc12m_train \
  -select_data \
  -start_path checkpoints/basemodel \
  -save_path checkpoints/iter1/ \
  -chunk 30 \
  -gpu_ids 0 1 2 3 \
  -iter 1
```
```shell
# finetuning on a original visual dialog dataset 
python train_gen.py \
  -mode vd_train \
  -continue \
  -start_path checkpoints/iter1/cc12m_train_30_3.ckpt \
  -save_path checkpoints/iter1/ \
  -chunk 30 \
  -gpu_ids 0 1 2 3
```

Adaptation to Discriminative Visual Dialog
-------------------------------------------
A ["discriminative" visual dialog model][8] requires answer candidates for each question, but our proposed approach only generates the ground-truth answer. Hence, we propose tricks to train the discriminative model. Based on the encoder-decoder model pre-trained on the synthetic dataset, we finetune the encoder model on the original visdial dataset. Please see our paper (Appendix B) for more details. 
```shell
python train_disc.py \
  -mode vd_train \
  -continue \
  -model enc_only_a \
  -batch_size 40 \
  -train_dense \
  -num_negative_samples 5 \
  -start_path checkpoints/x30_start_iter3.ckpt \
  -save_path checkpoints/disc \
  -chunk 30 \
  -gpu_ids 0 1 2 3
```


Visual Dialog Generation
------------------------
<img src="gst_vis.png" width="100%" align="middle"><br><br>
Visual dialog generation given image features and captions. The questioner and the teacher alternately generates the visual question and corresponding answer, respectively. 

You can generate **your own visual dialog dataset** just feeding [Bottom-up Attention Features][12] and the caption data. We extracted the image features using the [docker container][13].  
```shell
python generate.py \
  -mode cc12m_gen \
  -cc12m_image_feats data/cc12m/features/cc12m_img_feat_0.lmdb/ \
  -cc12m_caption data/cc12m/captions/cc12m_filtered_0.json \
  -start_path_q checkpoints/questioner_v1.0.ckpt \
  -start_path_a checkpoints/teacher_v1.0.ckpt \
  -save_name cc12m_dialogs_0.txt \
  -save_path data/gen_dialog \
  -gpu_ids 0 1
```


Evaluation
----------
Evaluation of the student model on VisDial v1.0 validation split. Validation scores can be checked in offline setting. But if you want to evaluate the model on the test dataset, you should change the mode to `vd_eval_test` and submit the text file to [EvalAI online evaluation server][11]. Also, evaluation for the VisDial v0.9 validation dataset is available. Please add `-vd_version 0.9`.
```shell
python evaluate_gen.py \
  -mode vd_eval_val \
  -start_path checkpoints/student_v1.0_iter3.ckpt \
  -save_path results \
  -save_name gen.txt \
  -gpu_ids 0 1 2 3
```
Evaluation for the discriminative model is as follows.
```shell
python evaluate_disc.py \
  -mode vd_eval_val \
  -start_path checkpoints/student_v1.0_iter3_disc_dense.ckpt \
  -save_path results \
  -save_name disc.txt \
  -gpu_ids 0 1 2 3
```


Adversarial Robustness Study
----------------------------
We propose three different adversarial attacks for VisDial: (1) the FGSM attack, (2) a coreference attack, and (3) a random token attack. The FGSM attack perturbs input visual features, and the others attack the dialog history (textual inputs).

Simply run below for the FGSM attack
```shell
python evaluate_gen_attack.py \
  -mode vd_eval_val \
  -attack fgsm \
  -start_path checkpoints/student_v1.0_iter3.ckpt \
  -save_path results \
  -save_name fgsm.txt \
  -gpu_ids 0 1 2 3
```

For the textual attacks, preprocessing is required.
Download the [counter-fitted word embeddings][14] and run the preprocessing code below.
```shell
python comp_cos_sim_mat.py counter-fitted-vectors.txt
```
Then, run the script
```shell
python evaluate_gen_attack.py \
  -mode vd_eval_val \
  -attack coreference \
  -visdial_processed_val data/visdial/visdial_1.0_val_crowdsourced.json \
  -visdial_processed_val_dense_annotations data/visdial/visdial_1.0_val_dense_annotations_processed_crowdsourced.json
  -start_path checkpoints/student_v1.0_iter3.ckpt \
  -save_path results \
  -save_name coreference.txt \
  -gpu_ids 0 1 2 3
```

Demo
----
We prepare interactive demo to show our model's generated answer easily. Simply run and enter the image id in [VisDial v1.0 validation images][15]. 
```shell
python inference.py
```




**Please leave a <font color='orange'>STAR ⭐</font> if you like this project!**

-->

[1]: https://arxiv.org/abs/2307.05963
[2]: https://ieee-iros.org/
[3]: https://gicheonkang.com
[4]: https://github.com/JHKim-snu/
[5]: https://github.com/qpwodlsqp/
[6]: https://bi.snu.ac.kr/~btzhang/
[7]: https://github.com/suyeonshin/
[8]: https://conda.io/docs/user-guide/install/download.html
