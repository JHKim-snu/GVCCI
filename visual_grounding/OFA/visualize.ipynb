{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VqwUvUcJ7uQ"
      },
      "source": [
        "## **OFA**\n",
        "Start to enjoy visual grounding with OFA! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVHcnR_yKJsF"
      },
      "source": [
        "## **Download Checkpoint**\n",
        "We provide a link for our public checkpoint, and you only need to wget it to your workspace. We also provide an alternative below. Choose one as you like!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrz9BgHBL0ew"
      },
      "source": [
        "## **Install Fairseq**\n",
        "We advise you to install fairseq by cloning the official repository and running \"pip install\". \n",
        "\n",
        "You should restart the window if you meet the hint of \"RESTART RUNTIME\". "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwdxKi3NL_m6"
      },
      "source": [
        "## **Preparation**\n",
        "Below you just need to import required packages, and check whether to use GPU or FP16. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1APBsXk2MC26"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from fairseq import utils, tasks\n",
        "from fairseq import checkpoint_utils\n",
        "from utils.eval_utils import eval_step\n",
        "from tasks.mm_tasks.refcoco import RefcocoTask\n",
        "from models.ofa import OFAModel\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3,4,5\"\n",
        "\n",
        "# Register refcoco task\n",
        "tasks.register_task('refcoco', RefcocoTask)\n",
        "\n",
        "# turn on cuda if GPU is available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "# use fp16 only when GPU is available\n",
        "use_fp16 = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8zara7cNf8v"
      },
      "source": [
        "## **Build Model**\n",
        "Below you can build your model and load the weights from the given checkpoint, and also build a generator. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYzxjbEzNew5",
        "outputId": "2e2990c7-7d84-4908-8f3c-3f7ebbfa6e5a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-15 02:14:06 | INFO | tasks.ofa_task | source dictionary: 59457 types\n",
            "2023-08-15 02:14:06 | INFO | tasks.ofa_task | target dictionary: 59457 types\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained ckpt & config\n",
        "overrides={\"bpe_dir\":\"utils/BPE\"}\n",
        "\n",
        "model_path = 'YOUR_MODEL_PATH.pt'\n",
        "\n",
        "\n",
        "models, cfg, task = checkpoint_utils.load_model_ensemble_and_task(\n",
        "        utils.split_paths(model_path),\n",
        "        arg_overrides=overrides\n",
        "    )\n",
        "\n",
        "cfg.common.seed = 7\n",
        "cfg.generation.beam = 5\n",
        "cfg.generation.min_len = 4\n",
        "cfg.generation.max_len_a = 0\n",
        "cfg.generation.max_len_b = 4\n",
        "cfg.generation.no_repeat_ngram_size = 3\n",
        "\n",
        "# Fix seed for stochastic decoding\n",
        "if cfg.common.seed is not None and not cfg.generation.no_seed_provided:\n",
        "    np.random.seed(cfg.common.seed)\n",
        "    utils.set_torch_seed(cfg.common.seed)\n",
        "\n",
        "# Move models to GPU\n",
        "for model in models:\n",
        "    model.eval()\n",
        "    if use_fp16:\n",
        "        model.half()\n",
        "    if use_cuda and not cfg.distributed_training.pipeline_model_parallel:\n",
        "        model.cuda()\n",
        "    model.prepare_for_inference_(cfg)\n",
        "\n",
        "# Initialize generator\n",
        "generator = task.build_generator(models, cfg.generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pot0EzT1SaPm"
      },
      "source": [
        "## **Preprocess**\n",
        "We demonstrate the required transformation fucntions for preprocessing inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq9akfBKSa3-",
        "outputId": "7823604e-96a6-4a48-a3f2-c00b883d20c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jhkim/anaconda3/envs/ofa/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# Image transform\n",
        "from torchvision import transforms\n",
        "mean = [0.5, 0.5, 0.5]\n",
        "std = [0.5, 0.5, 0.5]\n",
        "\n",
        "patch_resize_transform = transforms.Compose([\n",
        "    lambda image: image.convert(\"RGB\"),\n",
        "    transforms.Resize((cfg.task.patch_image_size, cfg.task.patch_image_size), interpolation=Image.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "# Text preprocess\n",
        "bos_item = torch.LongTensor([task.src_dict.bos()])\n",
        "eos_item = torch.LongTensor([task.src_dict.eos()])\n",
        "pad_idx = task.src_dict.pad()\n",
        "def encode_text(text, length=None, append_bos=False, append_eos=False):\n",
        "    s = task.tgt_dict.encode_line(\n",
        "        line=task.bpe.encode(text.lower()),\n",
        "        add_if_not_exist=False,\n",
        "        append_eos=False\n",
        "    ).long()\n",
        "    if length is not None:\n",
        "        s = s[:length]\n",
        "    if append_bos:\n",
        "        s = torch.cat([bos_item, s])\n",
        "    if append_eos:\n",
        "        s = torch.cat([s, eos_item])\n",
        "    return s\n",
        "\n",
        "# Construct input for refcoco task\n",
        "patch_image_size = cfg.task.patch_image_size\n",
        "def construct_sample(image: Image, text: str):\n",
        "    w, h = image.size\n",
        "    w_resize_ratio = torch.tensor(patch_image_size / w).unsqueeze(0)\n",
        "    h_resize_ratio = torch.tensor(patch_image_size / h).unsqueeze(0)\n",
        "    patch_image = patch_resize_transform(image).unsqueeze(0)\n",
        "    patch_mask = torch.tensor([True])\n",
        "    src_text = encode_text(' which region does the text \" {} \" describe?'.format(text), append_bos=True, append_eos=True).unsqueeze(0)\n",
        "    src_length = torch.LongTensor([s.ne(pad_idx).long().sum() for s in src_text])\n",
        "    sample = {\n",
        "        \"id\":np.array(['42']),\n",
        "        \"net_input\": {\n",
        "            \"src_tokens\": src_text,\n",
        "            \"src_lengths\": src_length,\n",
        "            \"patch_images\": patch_image,\n",
        "            \"patch_masks\": patch_mask,\n",
        "        },\n",
        "        \"w_resize_ratios\": w_resize_ratio,\n",
        "        \"h_resize_ratios\": h_resize_ratio,\n",
        "        \"region_coords\": torch.randn(1, 4)\n",
        "    }\n",
        "    return sample\n",
        "  \n",
        "# Function to turn FP32 to FP16\n",
        "def apply_half(t):\n",
        "    if t.dtype is torch.float32:\n",
        "        return t.to(dtype=torch.half)\n",
        "    return t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZHHeZ8XCCRK"
      },
      "source": [
        "## **Visualize from Test-E samples**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "test_path = '../../data/test/Test-E.pth'\n",
        "answer = torch.load(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-mvEomkCCqT",
        "outputId": "a581a034-fa80-4d44-a924-827987b51efe"
      },
      "outputs": [],
      "source": [
        "q = 0\n",
        "image = Image.open('../../data/test/Test-E/{}'.format(answer[q][0]))\n",
        "text = answer[q][3]\n",
        "sample = construct_sample(image, text)\n",
        "sample = utils.move_to_cuda(sample) if use_cuda else sample\n",
        "\n",
        "with torch.no_grad():\n",
        "    result, scores = eval_step(task, generator, models, sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "B1bRRE1lG_99",
        "outputId": "f32e33d9-178e-40fe-fbbd-2fdb5a8f6ed4"
      },
      "outputs": [],
      "source": [
        "img = cv2.cvtColor(numpy.asarray(image), cv2.COLOR_RGB2BGR)\n",
        "cv2.rectangle(\n",
        "    img,\n",
        "    (int(result[0][\"box\"][0]), int(result[0][\"box\"][1])),\n",
        "    (int(result[0][\"box\"][2]), int(result[0][\"box\"][3])),\n",
        "    (0, 255, 0),\n",
        "    3\n",
        ")\n",
        "\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)\n",
        "print(answer[q][0])\n",
        "print(text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ofa",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "4f1fbe006cad89593fbca439e0c3b8678ee1290e64aacc10652d97d83190b5e8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
